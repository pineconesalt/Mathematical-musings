{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9108a42d-4809-48d9-a95f-3817891f5c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534f04bd-00f6-4d5a-816f-0e3e37a713ac",
   "metadata": {},
   "source": [
    "# Statistics for Explority Data Analysis\n",
    "\n",
    "To understand data you first need to understand basic statistics how how to apply these to datasets. This notebook will discuss the main aspects of statistics which will be needed for basic data analysis. \n",
    "\n",
    "The statsitical analysis can be broadly broken down into the subsets:\n",
    "* Measures of Location and Dispersion\n",
    "* Estimates for variability\n",
    "* Data distribution\n",
    "* Correlation\n",
    "* Exploring variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3783e75e-dd4b-41fb-be32-ea2c385ef251",
   "metadata": {},
   "source": [
    "## Measures of Location and Dispersion\n",
    "\n",
    "This describes the central tendency of the data. It is a very simple method to help understand what the data looks like.\n",
    "\n",
    "### Mean (Average)\n",
    "The mean, denoted by $\\bar{x}$ is the sum of all sample values, $x_i$  divided by the sample size, $n$.\n",
    "$$\\frac{\\sum_{i=1}^n x_i}{n}$$\n",
    "The mean uses every data value, which in a statistical sense is efficient. The drawback is that outliers will negatively influence the  calculation.\n",
    "### Trimmed mean (truncated mean)\n",
    "To fix the issue with outliers when calculating the mean, the trimmed mean first removes a certain percentage of the smallest and largest values of the dataset. For instance, a 10% trimmed mean removes the top 10% and the bottom 10% in the dataset when ordered. The mean can then be calculated as normal.\n",
    "\n",
    "### Weighted average\n",
    "By giving each value a weight, $w$, or the importance, the average can be calculated by the values which matter the most. The higher the weight the higher the importance of that value.\n",
    "\n",
    "The weights may be given, for example; two school classes of 20 students and 30 students respectively where graded and the first class attained a mean score of 80%, whereas the second class attained a mean score of 90%. The weighted average for both classes would be\n",
    "$$\\frac{(80^*30)+(90^*20)}{30+20)}=86\\%$$\n",
    "\n",
    "\n",
    "The general formula for the weighted mean is:\n",
    "\n",
    "$$\\bar{x}=\\frac{\\sum_{i=1}^n w_ix_i}{\\sum_{i=1}^n x_i}$$\n",
    "\n",
    "The formulas are simplified when th weights are normalised, that is\n",
    "\n",
    "$$\\sum_{i=1}^n w'_i=1$$\n",
    "\n",
    "where $w'$ is the normalised weight. The normalised weights can be found by:\n",
    "\n",
    "$$w'_i = \\frac{w_i}{\\sum_{j=1}^n w_j}$$\n",
    "\n",
    "#### Variance-defined weights\n",
    "If the variance (mean-squared error), $\\sigma_i^2$ is known, then one possible choice for the weights is\n",
    "\n",
    "$$w_i=\\frac{1}{\\sigma_i^2}$$\n",
    "\n",
    "### Mode\n",
    "The mode is the most frequent value, or if the data is grouped, the group with the largest frequency. For continuous data, the value depends upon the accuracy of the data, that is how many significant figures are used. Therefore it is more important for catagorical data.\n",
    "\n",
    "Bimodal distribution is used to descibe distibutions that have two peaks. This can be caused by having two distinct populations, for example, height of the population which contain both mean and women.\n",
    "\n",
    "### Median\n",
    "This is defined as the middle point of the ordered data. It is immune to outliers, but as it does not use all the datapints it is statistically inefficient. It is mainly used for catagorical values where the mean cannot be used.\n",
    "\n",
    "### Outlier\n",
    "An outliers can be thought of as a value with a extrodinary range from all other values. They are either classed as univarate, where the outlier is represented within a single column, or multivarate, in which can only be seen in the joint combination of two or more variables. \n",
    "\n",
    "What constitutes an outlier is subjective and knowledge of the dataset is important to understand if a value is to be deemed an outlier. That said there are methods that can be used for identification, assuming the data is normally distributed. These are based on mean and standard deviation, and include:\n",
    "1. Chauvenet's criterion\n",
    "2. Grubbs's test for outliers\n",
    "3. Dixon's Q test\n",
    "4. ASTM E178 Standard Practice for Dealing With Outlying Observations\n",
    "5. Mahalanobis distance and leverage are often used to detect outliers, especially in the development of linear regression models.\n",
    "6. Subspace and correlation based techniques for high-dimensional numerical data.\n",
    "\n",
    "A nonparametric detection, for which the data does not need to be normally distributed is the Tukey fences.\n",
    "\n",
    "#### Turkey Fences\n",
    "\n",
    "Outliers can be found using the Interquartile Range (IQR). The IQR is calculated by the difference between the $75^{th}$ percentile and the $25^{th}$ percentile, that is $IQR=Q3-Q1$. If a datapoint lies outside a interquartile by 1.5 IQR then it can be denoted as an outlier\n",
    "$$\\text{low outlier range} < Q1-1.5 IQR$$\n",
    "$$\\text{high outlier range} > Q3+1.5 IQR$$\n",
    "\n",
    "Any value that is further away than $3IQR$ indicates that it is \"far out\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85759d7c-cc88-4d01-85aa-09dff281b82b",
   "metadata": {},
   "source": [
    "## Variability\n",
    "* Deviation (errors, residual)\n",
    "* Variance (mean-square error)\n",
    "* Standard deviation (Euclidean norm)\n",
    "The amount of variation of the samples from the mean.\n",
    "* Mean absolute deviation (manhatten norm)\n",
    "* Range\n",
    "* Percentile\n",
    "* Interquartile range\n",
    "\n",
    "### Comparing continuous data\n",
    "* scatter plots\n",
    "* heatmaps\n",
    "* hexagonal binnng\n",
    "* contour plots\n",
    "\n",
    "### Comparing catgorical data\n",
    "* contingency tables\n",
    "\n",
    "### Comparing continuous data with catgorical data\n",
    "* box plots\n",
    "* violin plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a13e3-33b2-47b0-8e0e-475ce8e77770",
   "metadata": {},
   "source": [
    "# Data and Sampling Distribution\n",
    "The main ideas to understand for data and sampling distribution are:\n",
    "* Sampling\n",
    "* Bias\n",
    "* Sampling distribution of a statistic\n",
    "* Bootstrap\n",
    "* Confidence intervals\n",
    "* Main types of distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3765566e-2826-4b92-93a3-d19577572dbf",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Sampling allows data analytics to be carried out when the original population is too big to compute or the data from the full population unknown. For example, if we wanted to know if the population preferred cats or dogs, ensuring that every person gave an answer would  take too long and involve too high a cost. Instead we might ask 1000 people which they prefer and use this to model the entire population. \n",
    "\n",
    "### Random Sampling\n",
    "Random sampling is a simple method that chooses samples from a large dataset at random. This means that no prior knowledge of the dataset is needed, and removes all bias from the choosing of the subset as all samples are equally likely to be choosen. \n",
    "\n",
    "The drawbacks are that a small dataset can incur sample selection bias. This leads a representation of the full population being skewed. For example, asking 1000 users of the subreddit r/cats if they prefer cats or dogs will lead to roughly 100% saying that they indeded prefer cats. Does this mean we can infer that everyone in the world thinks cats are better than dogs? \n",
    "\n",
    "### Stratified Sampling\n",
    "Dividing the population into homogenous subpopulations (called strata) based on spefic mutually exclusive charateristics, and choosing the equal samples from each subpopulation randomly ensures that characteristic is equally represented within the sample. This helps balancing the sampling sets when the populations characteristics are diverse. \n",
    "\n",
    "The main disadvatage is that if a characteristic has very small sample size, every other subpopulation sample size need to be as small to ensure equality. This leads to a small dataset which as said before can lead to other biases. \n",
    "\n",
    "## Bias\n",
    "These include: \n",
    "* **Selection bias** - where the study population is not representative of the target population.\n",
    "* **Self-selection bias** - occurs when patients volunteer to enroll into a particular study, or conversely drop out of a study for specific reasons, as opposed to radomly.\n",
    "* **Recall bias** - when sample values are retrieved from remembering pasrt hitorical events. These are an issue in questionnaires and interviews.\n",
    "* **Observer bias** - when beliefs or expectations of the observer can directly influence collection of the data\n",
    "* **Surviorship bias** - when datasets only consider the existing observations and not those that cease to exist\n",
    "* **Omitted variable bias** - when a model leaves out one or more relevant variables.\n",
    "* **Cause-effect bias** - correlation does not imply causation.\n",
    "* **Funding bias** - when the outcome is likely to favour the study's financial sponsor\n",
    "\n",
    "## Sampling distribution of a statistic\n",
    "### Central limit theorem\n",
    "This establishes that the sum of independent random samples their properly normalised sum tends towards a distributions, even if the original variables themselvs are not normally distributed. This implies that probabilistic and statistical methods that work for normal distributions can be applicable to problems involving other types of distributions.\n",
    "\n",
    "THE CLT is an asymptotic distribution, therefore only orks on large sample sizes. As the sample size becomes much larger the distribution of averages converges to a more normal and narrower Gaussian distribution curve.\n",
    "\n",
    "### Standard error\n",
    "The standard error is the aproximate standard deviation of a statistical sample population. It measures the accuracy of a sample distribution in regards to the population. For instance if the popluation of Europe has a mean age of 45.6, yet a sample of 1000 Europeans has a mean age of 43.8, then the difference is known as the standard error. \n",
    "\n",
    "For the case where the statistic is the sample mean, and samples are uncorrelated, the standard error is: \n",
    "\n",
    "$$ \\sigma_{bar{x}} = \\frac{\\sigma}{\\sqrt{n}}$$\n",
    "\n",
    "where $\\sigma$ is the standard deviation of the population distribution of that qantity and $n$ is the sample size.\n",
    "\n",
    "## Hypothesis testing\n",
    "Hypothesis testing assumes that if a large enough sample of the population is taken then the sampling distribution will be approximately normal and the standard deviation will be equal to the standard error. Ths gives the ability of using only one sample as long it's large enough. Knowing that the sample size is sufficiently large can be problematic, and with only one dataset the assumption cannot be tested. If the assumption is incorrect then any conclusion drawn will also be incorrect. For this reason data anlysis has moved toward Bootstrap sampling.\n",
    "\n",
    "## Boostrap Sampling\n",
    "Instead of having just one sample of the population as in the hypothesis testing, bootstrap sampling draws $m$ number of sub samples of $n$ size from the main sample, randomly with replacement. For instance, if we have a sample of size 1000, we can set the subsamples of size $n=100$. For the first subsample $m_1$ we select these at random. The subsample $m_2$ will also be selected out of the 1000 samples at random, and therefore may even contain some of the same samples as $m_1$. This technique is known as resampling. \n",
    "\n",
    "As the population is unknown, the error between the sample and the population is also unknown. Here we treat the sample as the population, statistics can now be measured beween this and the subsamples.  This allows for estimates of standard errors and confidence intervals, giving a way to control and check the stability of the results. \n",
    "\n",
    "Bootstraping is recomended when the theoretical distribution of a statistic is complicated or unown, when the sample size is insufficient for straight forward statistical interference, or when power calculations are to be performed, such as a confusion matrix.\n",
    "\n",
    "Bootstraping is not to be used if the underlying distribution is heavy-tailed, that is if the probaility distributions are not exponentially bounded. Below shows the Cauchy distribution being two side heavy, in which the line diverges from the exponential curve, staying above it.\n",
    "\n",
    "![cauchy distribution](http://www.maa.org/sites/default/files/images/upload_library/19/NormalCaucy.png)\n",
    "\n",
    "## Confidence Interval\n",
    "t-statistics  t-distribution\n",
    "$$\\text{ci for }\\mu = \\bar{x} \\pm z\\left(\\frac{\\sigma}{\\sqrt{n}}\\right)$$\n",
    "Understand basics but Bootstrap can do all these things\n",
    "\n",
    "## Main types of distribution\n",
    "* Normal distribution\n",
    "* Asymmetric (skewed) distribution\n",
    "* t distribution\n",
    "* Binomial\n",
    "* Poisson\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41778344-7a65-454c-8f20-37992d564bb5",
   "metadata": {},
   "source": [
    "# Advanced Statistics\n",
    "\n",
    "These ideas are used for machine learning and hypothesis testing, and include:\n",
    "* Statistical experiments\n",
    "* Significance testing\n",
    "* Regression\n",
    "* Statistical Machine Learning\n",
    "* Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af0e793-52f9-4e86-87c6-9ebbad5b2bf8",
   "metadata": {},
   "source": [
    "# Hypothesis testing\n",
    "not used much in data analysis nowadays, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8b21b-da21-4181-8e11-a8a29c0b2f7e",
   "metadata": {},
   "source": [
    "# Bayesian Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5629123-3701-4409-bbf9-67f1725ae21f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
